# Implementation Plan: Python Log Analyzer for FFB Diagnostics

## 1. Context

**Goal:** Create a Python-based log analyzer tool that parses telemetry logs generated by lmuFFB and produces diagnostic reports, statistics, and plots. The primary use case is diagnosing slope detection issues, but the tool should be extensible to other FFB effects.

**Version:** Standalone Python tool (not part of lmuFFB C++ codebase)  
**Date:** 2026-02-03  
**Priority:** Medium (Diagnostic infrastructure)

## 2. Reference Documents

- Telemetry Logger Plan: `docs/dev_docs/implementation_plans/plan_telemetry_logger.md`
- Investigation Report: `docs/dev_docs/investigations/slope_detection_issues_post_v071_investigation.md`
- Design Proposal: `docs/dev_docs/design proposal for a High-Performance Asynchronous Telemetry Logger.md`

---

## 3. Architecture Overview

### 3.1 Tool Structure

```
tools/
└── lmuffb_log_analyzer/
    ├── __init__.py
    ├── cli.py                 # Command-line interface
    ├── loader.py              # CSV loading and parsing
    ├── models.py              # Data models (Pydantic)
    ├── stats.py               # Statistical analysis functions
    ├── plots.py               # Matplotlib visualization
    ├── reports.py             # Text report generation
    ├── analyzers/
    │   ├── __init__.py
    │   ├── slope_analyzer.py  # Slope detection specific analysis
    │   ├── grip_analyzer.py   # Grip estimation analysis
    │   └── ffb_analyzer.py    # General FFB output analysis
    ├── requirements.txt       # Python dependencies
    └── README.md              # Usage documentation
```

### 3.2 Dependencies

```
# requirements.txt
pandas>=2.0.0
matplotlib>=3.7.0
numpy>=1.24.0
pydantic>=2.0.0
rich>=13.0.0       # For console output formatting
click>=8.1.0       # For CLI
```

---

## 4. Proposed Implementation

### 4.1 Data Models (`models.py`)

```python
from pydantic import BaseModel
from typing import Optional, List
from datetime import datetime

class SessionMetadata(BaseModel):
    """Extracted from log file header"""
    log_version: str
    timestamp: datetime
    app_version: str
    driver_name: str
    vehicle_name: str
    track_name: str
    
    # Settings snapshot
    gain: float
    understeer_effect: float
    sop_effect: float
    slope_enabled: bool
    slope_sensitivity: float
    slope_threshold: float
    slope_alpha_threshold: Optional[float] = None  # v0.7.3+
    slope_decay_rate: Optional[float] = None       # v0.7.3+

class AnalysisResult(BaseModel):
    """Container for analysis results"""
    session: SessionMetadata
    duration_seconds: float
    frame_count: int
    
    # General stats
    avg_speed_kmh: float
    max_lat_g: float
    clipping_percentage: float
    marker_count: int
    
    # Slope-specific stats
    slope_active_percentage: float
    slope_variance: float
    slope_range: tuple[float, float]
    grip_floor_percentage: float
    
    # Issues detected
    issues: List[str]
    warnings: List[str]

class MarkerEvent(BaseModel):
    """User-triggered marker"""
    timestamp: float
    frame_index: int
    context: dict  # Surrounding values
```

### 4.2 Log Loader (`loader.py`)

```python
import pandas as pd
from pathlib import Path
from typing import Tuple
from .models import SessionMetadata

def load_log(filepath: str) -> Tuple[SessionMetadata, pd.DataFrame]:
    """
    Load lmuFFB telemetry log file.
    
    Returns:
        Tuple of (SessionMetadata, DataFrame with telemetry data)
    """
    path = Path(filepath)
    if not path.exists():
        raise FileNotFoundError(f"Log file not found: {filepath}")
    
    # Parse header comments
    metadata = _parse_header(path)
    
    # Find data start line (first non-comment line)
    with open(path, 'r') as f:
        lines = f.readlines()
    
    data_start = 0
    for i, line in enumerate(lines):
        if not line.startswith('#'):
            data_start = i
            break
    
    # Load CSV data
    df = pd.read_csv(filepath, skiprows=data_start)
    
    return metadata, df

def _parse_header(path: Path) -> SessionMetadata:
    """Extract metadata from header comments"""
    header_data = {}
    
    with open(path, 'r') as f:
        for line in f:
            if not line.startswith('#'):
                break
            
            line = line.lstrip('# ').strip()
            if ':' in line:
                key, value = line.split(':', 1)
                header_data[key.strip().lower().replace(' ', '_')] = value.strip()
    
    return SessionMetadata(
        log_version=header_data.get('lmuffb_telemetry_log', 'unknown'),
        timestamp=_parse_datetime(header_data.get('date', '')),
        app_version=header_data.get('app_version', 'unknown'),
        driver_name=header_data.get('driver', 'Unknown'),
        vehicle_name=header_data.get('vehicle', 'Unknown'),
        track_name=header_data.get('track', 'Unknown'),
        gain=float(header_data.get('gain', 1.0)),
        understeer_effect=float(header_data.get('understeer_effect', 1.0)),
        sop_effect=float(header_data.get('sop_effect', 1.0)),
        slope_enabled=header_data.get('slope_detection', '').lower() == 'enabled',
        slope_sensitivity=float(header_data.get('slope_sensitivity', 0.5)),
        slope_threshold=float(header_data.get('slope_threshold', -0.3)),
        slope_alpha_threshold=_safe_float(header_data.get('slope_alpha_threshold')),
        slope_decay_rate=_safe_float(header_data.get('slope_decay_rate')),
    )
```

### 4.3 Slope Detection Analyzer (`analyzers/slope_analyzer.py`)

```python
import pandas as pd
import numpy as np
from typing import Dict, List, Any
from ..models import AnalysisResult

def analyze_slope_stability(df: pd.DataFrame, threshold: float = 0.02) -> Dict[str, Any]:
    """
    Analyze the stability of slope detection algorithm.
    
    Args:
        df: Telemetry DataFrame
        threshold: dAlpha/dt threshold for "active" calculation
    
    Returns:
        Dictionary with stability metrics
    """
    results = {}
    
    # Basic slope statistics
    slope = df['SlopeCurrent']
    results['slope_mean'] = slope.mean()
    results['slope_std'] = slope.std()
    results['slope_min'] = slope.min()
    results['slope_max'] = slope.max()
    results['slope_range'] = (slope.min(), slope.max())
    results['slope_variance'] = slope.var()
    
    # Percentage of time slope is actively calculated
    if 'dAlpha_dt' in df.columns:
        active_mask = np.abs(df['dAlpha_dt']) > threshold
        results['active_percentage'] = active_mask.mean() * 100
    else:
        results['active_percentage'] = None
    
    # Percentage of time at grip floor (0.2)
    if 'SlopeSmoothed' in df.columns:
        floor_mask = df['SlopeSmoothed'] <= 0.21
        results['floor_percentage'] = floor_mask.mean() * 100
    elif 'GripFactor' in df.columns:
        floor_mask = df['GripFactor'] <= 0.21
        results['floor_percentage'] = floor_mask.mean() * 100
    else:
        results['floor_percentage'] = None
    
    # Grip on straights analysis
    straight_mask = (
        (df['Speed'] > 27.8) &  # > 100 km/h
        (np.abs(df.get('SlipAngleFront', df.get('calc_slip_angle_front', 0))) < 0.02)
    )
    if straight_mask.any():
        grip_col = 'GripFactor' if 'GripFactor' in df.columns else 'SlopeSmoothed'
        results['grip_on_straights_mean'] = df.loc[straight_mask, grip_col].mean()
        results['grip_on_straights_std'] = df.loc[straight_mask, grip_col].std()
    else:
        results['grip_on_straights_mean'] = None
        results['grip_on_straights_std'] = None
    
    # Issue detection
    results['issues'] = []
    
    if results['slope_std'] > 5.0:
        results['issues'].append(
            f"HIGH SLOPE VARIANCE ({results['slope_std']:.2f}) - Algorithm may be unstable"
        )
    
    if results.get('floor_percentage', 0) > 5.0:
        results['issues'].append(
            f"FREQUENT FLOOR HITS ({results['floor_percentage']:.1f}%) - Algorithm too aggressive"
        )
    
    if results.get('active_percentage') is not None and results['active_percentage'] < 30.0:
        results['issues'].append(
            f"LOW ACTIVE PERCENTAGE ({results['active_percentage']:.1f}%) - Slope rarely calculated"
        )
    
    if results.get('grip_on_straights_mean') is not None and results['grip_on_straights_mean'] < 0.9:
        results['issues'].append(
            f"LOW GRIP ON STRAIGHTS ({results['grip_on_straights_mean']:.2f}) - Slope stuck at negative"
        )
    
    return results

def detect_oscillation_events(
    df: pd.DataFrame, 
    column: str = 'SlopeCurrent',
    threshold: float = 5.0,
    min_duration: float = 0.1
) -> List[Dict[str, Any]]:
    """
    Detect periods where a signal oscillates rapidly between extremes.
    
    Returns:
        List of oscillation events with start time, duration, amplitude
    """
    events = []
    
    if column not in df.columns:
        return events
    
    signal = df[column].values
    time = df['Time'].values if 'Time' in df.columns else np.arange(len(signal)) * 0.01
    
    # Calculate rolling std to detect high-variance periods
    window = 50  # 0.5 seconds at 100Hz
    rolling_std = pd.Series(signal).rolling(window, center=True).std().values
    
    # Find periods where std > threshold
    in_event = False
    event_start = 0
    
    for i, std in enumerate(rolling_std):
        if std > threshold and not in_event:
            in_event = True
            event_start = i
        elif (std <= threshold or np.isnan(std)) and in_event:
            in_event = False
            duration = time[i] - time[event_start]
            if duration >= min_duration:
                events.append({
                    'start_time': time[event_start],
                    'end_time': time[i],
                    'duration': duration,
                    'amplitude': np.abs(signal[event_start:i]).max(),
                    'frame_start': event_start,
                    'frame_end': i
                })
    
    return events

def analyze_grip_correlation(df: pd.DataFrame) -> Dict[str, float]:
    """
    Analyze correlation between calculated grip and expected physics.
    """
    results = {}
    
    grip_col = 'GripFactor' if 'GripFactor' in df.columns else 'SlopeSmoothed'
    
    if grip_col in df.columns:
        # Correlation with absolute slip angle
        if 'calc_slip_angle_front' in df.columns:
            slip = np.abs(df['calc_slip_angle_front'])
            results['grip_vs_slip_correlation'] = -df[grip_col].corr(slip)
        
        # Correlation with lateral G
        if 'LatAccel' in df.columns:
            lat_g = np.abs(df['LatAccel'])
            results['grip_vs_latg_correlation'] = df[grip_col].corr(lat_g)
    
    return results
```

### 4.4 Plotting (`plots.py`)

```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Optional

def plot_slope_timeseries(
    df: pd.DataFrame, 
    output_path: Optional[str] = None,
    show: bool = True
) -> str:
    """
    Generate 4-panel time-series plot for slope detection analysis.
    """
    fig, axes = plt.subplots(4, 1, figsize=(14, 12), sharex=True)
    fig.suptitle('Slope Detection Analysis - Time Series', fontsize=14, fontweight='bold')
    
    time = df['Time'] if 'Time' in df.columns else np.arange(len(df)) * 0.01
    
    # Panel 1: Inputs (Lat G and Slip Angle)
    ax1 = axes[0]
    ax1.plot(time, df['LatAccel'] / 9.81, label='Lateral G', color='#2196F3', alpha=0.8)
    ax1.set_ylabel('Lateral G', color='#2196F3')
    ax1.tick_params(axis='y', labelcolor='#2196F3')
    ax1.legend(loc='upper left')
    ax1.grid(True, alpha=0.3)
    
    ax1_twin = ax1.twinx()
    if 'calc_slip_angle_front' in df.columns:
        ax1_twin.plot(time, df['calc_slip_angle_front'], label='Slip Angle', 
                      color='#FF9800', alpha=0.8)
    ax1_twin.set_ylabel('Slip Angle (rad)', color='#FF9800')
    ax1_twin.tick_params(axis='y', labelcolor='#FF9800')
    ax1_twin.legend(loc='upper right')
    ax1.set_title('Inputs: Lateral G and Slip Angle')
    
    # Panel 2: Derivatives
    ax2 = axes[1]
    if 'dG_dt' in df.columns:
        ax2.plot(time, df['dG_dt'], label='dG/dt', color='#2196F3', alpha=0.8)
    if 'dAlpha_dt' in df.columns:
        ax2.plot(time, df['dAlpha_dt'], label='dAlpha/dt', color='#FF9800', alpha=0.8)
        ax2.axhline(0.02, color='#F44336', linestyle='--', alpha=0.5, label='Threshold (0.02)')
        ax2.axhline(-0.02, color='#F44336', linestyle='--', alpha=0.5)
    ax2.set_ylabel('Derivative')
    ax2.legend(loc='upper right')
    ax2.grid(True, alpha=0.3)
    ax2.set_title('Derivatives: dG/dt and dAlpha/dt')
    
    # Panel 3: Slope
    ax3 = axes[2]
    if 'SlopeCurrent' in df.columns:
        ax3.plot(time, df['SlopeCurrent'], label='Slope (dG/dAlpha)', color='#9C27B0', linewidth=0.8)
        ax3.axhline(-0.3, color='#F44336', linestyle='--', alpha=0.5, label='Neg Threshold (-0.3)')
        ax3.axhline(0, color='#4CAF50', linestyle='-', alpha=0.3)
    ax3.set_ylabel('Slope (G/rad)')
    ax3.set_ylim(-15, 15)  # Clamp for visibility
    ax3.legend(loc='upper right')
    ax3.grid(True, alpha=0.3)
    ax3.set_title('Calculated Slope (dG/dAlpha)')
    
    # Panel 4: Grip Output
    ax4 = axes[3]
    grip_col = 'GripFactor' if 'GripFactor' in df.columns else 'SlopeSmoothed'
    if grip_col in df.columns:
        ax4.plot(time, df[grip_col], label='Grip Factor', color='#4CAF50', linewidth=1.0)
        ax4.axhline(0.2, color='#9E9E9E', linestyle='--', alpha=0.5, label='Floor (0.2)')
        ax4.axhline(1.0, color='#9E9E9E', linestyle='--', alpha=0.5)
    ax4.set_ylabel('Grip Factor')
    ax4.set_xlabel('Time (s)')
    ax4.set_ylim(0, 1.1)
    ax4.legend(loc='upper right')
    ax4.grid(True, alpha=0.3)
    ax4.set_title('Output: Grip Factor')
    
    # Add markers if present
    if 'Marker' in df.columns:
        marker_times = time[df['Marker'] == 1]
        for ax in axes:
            for mt in marker_times:
                ax.axvline(mt, color='#E91E63', linestyle='-', alpha=0.7, linewidth=2)
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        return output_path
    
    if show:
        plt.show()
    
    return ""

def plot_slip_vs_latg(
    df: pd.DataFrame,
    output_path: Optional[str] = None,
    show: bool = True
) -> str:
    """
    Scatter plot of Slip Angle vs Lateral G (tire curve visualization).
    """
    fig, ax = plt.subplots(figsize=(10, 8))
    
    slip_col = 'calc_slip_angle_front' if 'calc_slip_angle_front' in df.columns else None
    if slip_col is None:
        return ""
    
    slip = np.abs(df[slip_col])
    lat_g = np.abs(df['LatAccel'] / 9.81) if 'LatAccel' in df.columns else None
    
    if lat_g is None:
        return ""
    
    # Color by speed
    speed = df['Speed'] * 3.6 if 'Speed' in df.columns else None
    
    scatter = ax.scatter(slip, lat_g, c=speed, cmap='viridis', alpha=0.3, s=2)
    
    ax.set_xlabel('Slip Angle (rad)')
    ax.set_ylabel('Lateral G')
    ax.set_title('Tire Curve: Slip Angle vs Lateral G')
    ax.grid(True, alpha=0.3)
    
    if speed is not None:
        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label('Speed (km/h)')
    
    # Mark the theoretical peak region
    ax.axvline(0.08, color='#F44336', linestyle='--', alpha=0.5, label='Typical Peak (~0.08 rad)')
    ax.legend()
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        return output_path
    
    if show:
        plt.show()
    
    return ""

def plot_dalpha_histogram(
    df: pd.DataFrame,
    output_path: Optional[str] = None,
    show: bool = True
) -> str:
    """
    Histogram of dAlpha/dt values (shows when slope calculation is "active").
    """
    if 'dAlpha_dt' not in df.columns:
        return ""
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    dalpha = df['dAlpha_dt'].values
    
    # Create histogram
    ax.hist(dalpha, bins=100, color='#2196F3', alpha=0.7, edgecolor='white')
    
    # Mark the threshold
    ax.axvline(0.02, color='#F44336', linestyle='--', linewidth=2, label='Threshold (+0.02)')
    ax.axvline(-0.02, color='#F44336', linestyle='--', linewidth=2, label='Threshold (-0.02)')
    
    # Calculate percentages
    above_threshold = (np.abs(dalpha) > 0.02).mean() * 100
    
    ax.set_xlabel('dAlpha/dt (rad/s)')
    ax.set_ylabel('Frequency')
    ax.set_title(f'Distribution of dAlpha/dt\n{above_threshold:.1f}% of frames above threshold (active calculation)')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        return output_path
    
    if show:
        plt.show()
    
    return ""
```

### 4.5 CLI Interface (`cli.py`)

```python
import click
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

from .loader import load_log
from .analyzers.slope_analyzer import analyze_slope_stability, detect_oscillation_events
from .plots import plot_slope_timeseries, plot_slip_vs_latg, plot_dalpha_histogram
from .reports import generate_text_report

console = Console()

@click.group()
@click.version_option(version='1.0.0')
def cli():
    """lmuFFB Log Analyzer - Analyze FFB telemetry logs for diagnostics."""
    pass

@cli.command()
@click.argument('logfile', type=click.Path(exists=True))
def info(logfile):
    """Display session info from a log file."""
    metadata, df = load_log(logfile)
    
    console.print(Panel.fit(
        f"[bold blue]Session Information[/bold blue]\n\n"
        f"Driver: {metadata.driver_name}\n"
        f"Vehicle: {metadata.vehicle_name}\n"
        f"Track: {metadata.track_name}\n"
        f"Duration: {df['Time'].max():.1f} seconds\n"
        f"Frames: {len(df)}\n"
        f"App Version: {metadata.app_version}",
        title="Log File Info"
    ))

@cli.command()
@click.argument('logfile', type=click.Path(exists=True))
@click.option('--verbose', '-v', is_flag=True, help='Show detailed output')
def analyze(logfile, verbose):
    """Analyze a log file and show summary."""
    console.print(f"[bold]Analyzing:[/bold] {logfile}")
    
    metadata, df = load_log(logfile)
    
    # Run slope analysis
    slope_results = analyze_slope_stability(df)
    oscillations = detect_oscillation_events(df)
    
    # Display results
    table = Table(title="Slope Detection Analysis")
    table.add_column("Metric", style="cyan")
    table.add_column("Value", style="green")
    table.add_column("Status", style="yellow")
    
    table.add_row(
        "Slope Variance",
        f"{slope_results['slope_std']:.2f}",
        "⚠️ HIGH" if slope_results['slope_std'] > 5.0 else "✅ OK"
    )
    table.add_row(
        "Slope Range",
        f"{slope_results['slope_min']:.1f} to {slope_results['slope_max']:.1f}",
        "⚠️ WIDE" if (slope_results['slope_max'] - slope_results['slope_min']) > 20 else "✅ OK"
    )
    table.add_row(
        "Active %",
        f"{slope_results.get('active_percentage', 'N/A'):.1f}%" if slope_results.get('active_percentage') else "N/A",
        "⚠️ LOW" if slope_results.get('active_percentage', 100) < 30 else "✅ OK"
    )
    table.add_row(
        "Floor Hits",
        f"{slope_results.get('floor_percentage', 'N/A'):.1f}%" if slope_results.get('floor_percentage') else "N/A",
        "⚠️ HIGH" if slope_results.get('floor_percentage', 0) > 5 else "✅ OK"
    )
    table.add_row(
        "Oscillation Events",
        str(len(oscillations)),
        "⚠️ MANY" if len(oscillations) > 3 else "✅ OK"
    )
    
    console.print(table)
    
    # Show issues
    if slope_results['issues']:
        console.print("\n[bold red]Issues Detected:[/bold red]")
        for issue in slope_results['issues']:
            console.print(f"  • {issue}")

@cli.command()
@click.argument('logfile', type=click.Path(exists=True))
@click.option('--output', '-o', default='.', help='Output directory for plots')
@click.option('--all', 'plot_all', is_flag=True, help='Generate all plot types')
def plots(logfile, output, plot_all):
    """Generate diagnostic plots from a log file."""
    console.print(f"[bold]Generating plots for:[/bold] {logfile}")
    
    metadata, df = load_log(logfile)
    output_dir = Path(output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    base_name = Path(logfile).stem
    
    # Time series plot
    ts_path = output_dir / f"{base_name}_timeseries.png"
    plot_slope_timeseries(df, str(ts_path), show=False)
    console.print(f"  ✅ Created: {ts_path}")
    
    if plot_all:
        # Tire curve
        tc_path = output_dir / f"{base_name}_tire_curve.png"
        plot_slip_vs_latg(df, str(tc_path), show=False)
        console.print(f"  ✅ Created: {tc_path}")
        
        # dAlpha histogram
        hist_path = output_dir / f"{base_name}_dalpha_hist.png"
        plot_dalpha_histogram(df, str(hist_path), show=False)
        console.print(f"  ✅ Created: {hist_path}")
    
    console.print("\n[bold green]Done![/bold green]")

@cli.command()
@click.argument('logfile', type=click.Path(exists=True))
@click.option('--output', '-o', help='Output file path')
def report(logfile, output):
    """Generate a full diagnostic report."""
    metadata, df = load_log(logfile)
    
    report_text = generate_text_report(metadata, df)
    
    if output:
        with open(output, 'w') as f:
            f.write(report_text)
        console.print(f"[bold green]Report saved to:[/bold green] {output}")
    else:
        console.print(report_text)

if __name__ == '__main__':
    cli()
```

---

## 5. Test Plan

### 5.1 Unit Tests

#### Test 1: `test_loader_parse_header`
**Description:** Verify header parsing extracts all metadata fields.

#### Test 2: `test_loader_parse_data`
**Description:** Verify CSV data is loaded into DataFrame correctly.

#### Test 3: `test_slope_analyzer_stability`
**Description:** Verify slope stability metrics are calculated correctly.

**Test Data:** Create synthetic log with known characteristics.

#### Test 4: `test_oscillation_detection`
**Description:** Verify oscillation events are detected.

#### Test 5: `test_plot_generation`
**Description:** Verify plots are generated without errors.

### 5.2 Integration Tests

#### Test 6: `test_cli_workflow`
**Description:** Verify CLI commands work end-to-end.

---

## 6. Deliverables Checklist

### 6.1 Code
- [ ] Create `tools/lmuffb_log_analyzer/` directory structure
- [ ] Implement `loader.py` with header and data parsing
- [ ] Implement `models.py` with Pydantic models
- [ ] Implement `analyzers/slope_analyzer.py`
- [ ] Implement `plots.py` with matplotlib visualizations
- [ ] Implement `reports.py` for text report generation
- [ ] Implement `cli.py` with Click-based CLI
- [ ] Create `requirements.txt`
- [ ] Create `README.md` with usage instructions

### 6.2 Tests
- [ ] Create `tests/test_loader.py`
- [ ] Create `tests/test_analyzers.py`
- [ ] All tests pass

### 6.3 Documentation
- [ ] Update project README to mention analyzer tool
- [ ] Create usage examples in README.md

### 6.4 Implementation Notes

**Unforeseen Issues:**
- **Unicode Encoding Errors:** The CLI initially used emojis (✅, ⚠️) for status indicators. This caused `UnicodeEncodeError` on Windows when output was redirected or run in legacy consoles. Replaced with ASCII `[OK]`, `[HIGH]`, etc.
- **Python Import Path:** Relative imports in tests required running pytest with the `-m` flag from the `tools` directory to correctly resolve the `lmuffb_log_analyzer` package.
- **Matplotlib Backend:** Added `show=False` to plot generation in CLI/tests to ensure it works in non-interactive environments without crashing.

**Plan Deviations:**
- Changed emoji status indicators to ASCII text for reliability on Windows.
- Added explicitly typed conversions (e.g., `float()`, `int()`) in `slope_analyzer.py` to ensure Pydantic models receive standard Python types from Numpy/Pandas.

**Challenges Encountered:**
- Debugging redirected output encoding issues on Windows required converting temporary output files to UTF-8 for inspection.

**Recommendations:**
- Future CLI tools for Windows should avoid non-ASCII characters in primary output paths unless UTF-8 support is guaranteed.

---

## 7. Usage Examples

### Basic Analysis
```bash
cd tools/lmuffb_log_analyzer
python -m lmuffb_log_analyzer analyze path/to/log.csv
```

### Generate Plots
```bash
python -m lmuffb_log_analyzer plots path/to/log.csv --output ./plots --all
```

### Full Report
```bash
python -m lmuffb_log_analyzer report path/to/log.csv --output analysis_report.txt
```

---

*Plan Created: 2026-02-03*
*Status: Completed (2026-02-04)*
